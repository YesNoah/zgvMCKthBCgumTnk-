{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7e45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries and packages\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ccf43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631e5450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\noahc\\\\VC_Repo'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135b671c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.fake_voice_detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake_voice_detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeVoiceDetectionModel\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models.fake_voice_detection'"
     ]
    }
   ],
   "source": [
    "from models.fake_voice_detection.model import FakeVoiceDetectionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd495970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load audio files and label them\n",
    "human_path = os.path.dirname(os.getcwd()) + \"/data/raw/cv_delta/clips/*.wav\" # change this according to your folder name\n",
    "ai_path = os.path.dirname(os.getcwd()) + \"/data/raw/cv_delta/AIclips/*.wav\" # change this according to your folder name\n",
    "\n",
    "human_files = glob.glob(human_path)\n",
    "ai_files = glob.glob(ai_path)\n",
    "\n",
    "human_labels = [0] * len(human_files) # 0 means human voice\n",
    "ai_labels = [1] * len(ai_files) # 1 means fake voice\n",
    "\n",
    "audio_files = human_files + ai_files # concatenate two lists of files\n",
    "audio_labels = human_labels + ai_labels # concatenate two lists of labels\n",
    "\n",
    "df = pd.DataFrame({\"file\": audio_files, \"label\": audio_labels}) # create a dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract features from audio files\n",
    "def extract_features(file):\n",
    "    y, sr = librosa.load(file) # load an audio file\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr) # extract MFCC feature\n",
    "    mfccs_scaled = np.mean(mfccs.T,axis=0) # take the mean of each column\n",
    "    \n",
    "    return mfccs_scaled\n",
    "\n",
    "features = []\n",
    "\n",
    "for file in df[\"file\"]:\n",
    "    features.append(extract_features(file)) # apply the function to each file\n",
    "    \n",
    "features = np.array(features) # convert list to array\n",
    "\n",
    "features = features.reshape(features.shape[0], features.shape[1], 1) # reshape array for model input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load pre-trained model and make predictions\n",
    "model_path = \"fake_voice_detection_model.pth\" # change this according to your model path\n",
    "\n",
    "model = FakeVoiceDetectionModel() \n",
    "model.load_state_dict(torch.load(model_path)) \n",
    "model.eval() \n",
    "\n",
    "predictions = []\n",
    "\n",
    "for feature in features:\n",
    "    feature_tensor = torch.from_numpy(feature).float() \n",
    "    output_tensor = model(feature_tensor.unsqueeze(0)) \n",
    "    output_prob = torch.sigmoid(output_tensor).item() \n",
    "    output_label = int(output_prob > 0.5) \n",
    "    \n",
    "    predictions.append(output_label)\n",
    "\n",
    "accuracy = np.mean(predictions == df[\"label\"]) \n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save or display predictions and labels\n",
    "output_df = pd.DataFrame({\"file\": df[\"file\"], \"label\": df[\"label\"], \"prediction\": predictions})\n",
    "\n",
    "output_df.to_csv(\"output.csv\", index=False) \n",
    "\n",
    "print(output_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
